{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PdfFileReader' from 'PyPDF2' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd487faf44db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPyPDF2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPdfFileReader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPdfFileWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PdfFileReader' from 'PyPDF2' (unknown location)"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re \n",
    "\n",
    "\n",
    "from PyPDF2 import PdfFileReader,PdfFileWriter\n",
    "from bs4 import BeautifulSoup  \n",
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(date1, date2):\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        yield date1 + timedelta(n)\n",
    "        \n",
    "start_date = date(2020, 1, 1)\n",
    "end_date = date(2020, 1, 3)\n",
    "end_date = end_date  + datetime.timedelta(days=1)\n",
    "print(f'Start Date is {start_date}')\n",
    "print(f'End Date is {end_date}')\n",
    "\n",
    "\n",
    "print ('---------Download Date List  ------')\n",
    "weekdays = [6,7]\n",
    "effective_date_list = []\n",
    "for dt in daterange(start_date, end_date):\n",
    "    if dt.isoweekday() not in weekdays:\n",
    "        \n",
    "        # effective_date is 'datetime.date' type, so transfer to Str\n",
    "        timestampStr = dt.strftime(\"%Y-%m-%d\")\n",
    "        effective_date_list.append(timestampStr)\n",
    "print(effective_date_list)\n",
    "print ('------------------------------------')\n",
    "\n",
    "# Generate first page websites list\n",
    "# http://digitalpaper.stdaily.com/http_www.kjrb.com/kjrb/html/2020-01/23/node_2.htm\n",
    "paper_stdaily_base = \"http://digitalpaper.stdaily.com/http_www.kjrb.com/kjrb/html/\"\n",
    "tmp_str='2020-01-23'\n",
    "\n",
    "paper_fetch_list = []\n",
    "for idx_date in effective_date_list:\n",
    "    paper_current_url = paper_stdaily_base + idx_date[0:7] + '/' + idx_date[8:10] + '/node_2.htm'\n",
    "    paper_fetch_list.append(paper_current_url)\n",
    "\n",
    "# fetch website of eachday\n",
    "async def download_one(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as resp: \n",
    "            return await resp.text()\n",
    "\n",
    "async def download_all(effective_date_list, sites):\n",
    "    tasks = [asyncio.create_task(download_one(site)) for site in sites]\n",
    "    texts = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # generate downloads lists without 404\n",
    "    download_pdf_name_lists = []\n",
    "    for date_item, text_item in zip(effective_date_list, texts):\n",
    "        soup_item = BeautifulSoup(text_item,'html.parser')\n",
    "        download_tag = soup_item.find_all('title')\n",
    "        # http://digitalpaper.stdaily.com/http_www.kjrb.com/kjrb/images/2020-01/23/01/KJRB2020012301.pdf\n",
    "        # http://digitalpaper.stdaily.com/http_www.kjrb.com/kjrb/images/2020-01/23/02/KJRB2020012302.pdf\n",
    "        if '404' not in str(download_tag):\n",
    "            print(f'{date_item} has news.')\n",
    "            url_base = 'http://digitalpaper.stdaily.com/http_www.kjrb.com/kjrb/images/'\n",
    "            # Download each page of the date\n",
    "            for page_number in range(1,9):\n",
    "                download_url = url_base + date_item[0:7] + '/' + date_item[8:10] + '/' + '{:02d}'.format(page_number) + '/'+ 'KJRB' + date_item[0:4] + date_item[5:7] + date_item[8:10] + '{:02d}'.format(page_number) + '.pdf'\n",
    "                \n",
    "                current_file_name = 'KJRB' + date_item[0:4] + date_item[5:7] + date_item[8:10] + '{:02d}'.format(page_number) + \"_pdf.pdf\"\n",
    "                myfile = requests.get(download_url, allow_redirects=True)\n",
    "                open(current_file_name, 'wb').write(myfile.content)\n",
    "                download_pdf_name_lists.append(current_file_name)\n",
    "                print(f'--- Finish downloading {current_file_name} file')\n",
    "            pdf_output = PdfFileWriter()\n",
    "            for infn in download_pdf_name_lists:\n",
    "                #print infn\n",
    "                pdf_input = PdfFileReader(open(infn, 'rb'), strict=False)\n",
    "                # 获取 pdf 共用多少页\n",
    "                page_count = pdf_input.getNumPages()\n",
    "                #print(f'The number of pages to merge is : {page_count} page')\n",
    "                for i in range(page_count):\n",
    "                    pdf_output.addPage(pdf_input.getPage(i))\n",
    "            summaryName = date_item[0:4] + date_item[5:7] + date_item[8:10] +'_KJRB_Summary.pdf'\n",
    "            pdf_output.write(open(summaryName, 'wb'))\n",
    "            print(f'|========> Finish merge {summaryName} file')\n",
    "    return\n",
    "\n",
    "tmp_str1 = 'http://digitalpaper.stdaily.com/http_www.kjrb.com/kjrb/html/2020-01/23/node_2.htm'\n",
    "tmp_str2 = 'http://digitalpaper.stdaily.com/http_www.kjrb.com/kjrb/html/2020-01/22/node_2.htm'\n",
    "#paper_fetch_list = []\n",
    "#paper_fetch_list.append(tmp_str1)\n",
    "#paper_fetch_list.append(tmp_str2)\n",
    "\n",
    "#print(paper_fetch_list)\n",
    "print(len(paper_fetch_list))\n",
    "print(len(effective_date_list))\n",
    "asyncio.run(download_all(effective_date_list, paper_fetch_list)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
